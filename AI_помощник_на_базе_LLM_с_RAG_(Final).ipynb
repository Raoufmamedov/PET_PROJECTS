{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raoufmamedov/PET_PROJECTS/blob/main/AI_%D0%BF%D0%BE%D0%BC%D0%BE%D1%89%D0%BD%D0%B8%D0%BA_%D0%BD%D0%B0_%D0%B1%D0%B0%D0%B7%D0%B5_LLM_%D1%81_RAG_(Final).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-"
      ],
      "metadata": {
        "id": "AQJOxhXffDRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Jupyter Notebook: AI-помощник на базе LLM с RAG для запросов к базе знаний**\n",
        "\n",
        "## **1. Введение**\n",
        "\n",
        "### Данный ноутбук посвящен разработке интеллектуального помощника, использующего большие языковые модели (LLM) и Retrieval-Augmented Generation (RAG) для ответов на вопросы на основе предоставленной базы знаний.\n",
        "\n",
        "### **Бизнес-проблема:** Компании часто имеют обширную внутреннюю документацию, к которой трудно получить быстрый и точный доступ. AI-помощник с RAG может предоставлять контекстно-зависимые ответы, улучшая эффективность и удовлетворенность.\n",
        "\n",
        "### **Цель проекта:** Построить RAG-систему, способную отвечать на вопросы на основе реального текстового корпуса, используя бесплатные ресурсы Google Colab. В качестве примера мы сформируем базу знаний из англоязычного текста Библии короля Якова (KJB), опубликованного на сайте проекта Project Gutenberg. Это большой корпус, состаящий из 39 книг, и он хорошо подходит в качестве коллекции документов различающихся стилем и манерой повествования, а также содержащих упоминание разных личностей имеющих одно и то же имя.\n",
        "\n",
        "### Источник данных: \"The King James Version of the Bible\" (Библия короля Якова)\n",
        "### URL: https://www.gutenberg.org/files/10/10-0.txt"
      ],
      "metadata": {
        "id": "Crw6YSg7eqlQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOJp2rolzIQf"
      },
      "outputs": [],
      "source": [
        "# ## 2. установка и импорт необходимых библиотек, настройка среды\n",
        "# Здесь мы импортируем все библиотеки, которые потребуются для проекта.\n",
        "# Если ноутбук запускается в Google Colab, убедитесь, что используется среду с GPU (T4).\n",
        "\n",
        "!pip install transformers sentence-transformers langchain langchain-community faiss-cpu accelerate bitsandbytes -q\n",
        "!pip install pypdf -q # Для чтения PDF, если понадобится\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBo8sXg3Z8jj"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import os\n",
        "import requests # Для загрузки данных\n",
        "from tqdm.notebook import tqdm # Для прогресс-баров\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Для RAG\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.prompts import PromptTemplate\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "import torch\n",
        "from langchain_core.documents import Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6l2cM8TBXMVN"
      },
      "outputs": [],
      "source": [
        "# Проверяем доступность GPU\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU доступен: {torch.cuda.get_device_name(0)}\")\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    print(\"GPU не доступен, будет использоваться CPU.\")\n",
        "    device = \"cpu\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Осуществляем загрузку данных и их очистку с удалением метаданных и сохраняем текст для дальнейшей обработки\n"
      ],
      "metadata": {
        "id": "D6EQ5pMThF5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Загрузка текста книги из Project Gutenberg...\")\n",
        "book_url = \"https://www.gutenberg.org/files/10/10-0.txt\"\n",
        "response = requests.get(book_url)\n",
        "response.raise_for_status() # Проверка, успешности запроса\n",
        "\n",
        "raw_text = response.text\n",
        "print(f\"Текст успешно загружен. Размер текста: {len(raw_text)} символов.\")\n",
        "\n",
        "# Очистка текста: удаляем метаданные Project Gutenberg в начале и конце\n",
        "start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK 10 ***\"\n",
        "end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK 10 ***\"\n",
        "\n",
        "start_index = raw_text.find(start_marker)\n",
        "end_index = raw_text.find(end_marker)\n",
        "\n",
        "if start_index!= -1 and end_index!= -1:\n",
        "    cleaned_text = raw_text[start_index + len(start_marker):end_index].strip()\n",
        "    print(f\"Текст очищен от метаданных. Размер очищенного текста: {len(cleaned_text)} символов.\")\n",
        "else:\n",
        "    cleaned_text = raw_text.strip()\n",
        "    print(\"Метаданные не найдены, используется весь текст.\")\n",
        "\n",
        "# Сохраняем очищенный текст в файл для дальнейшей обработки\n",
        "with open(\"KJB.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(cleaned_text)\n",
        "print(\"Очищенный текст сохранен в 'KJB.txt'.\")"
      ],
      "metadata": {
        "id": "HyQ-_ZFMhB9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Предварительная обработка текста и генерация эмбеддингов**\n",
        "\n",
        "### На этом этапе мы подготовим текстовые данные для RAG-системы: чанкируем текст, сгенерируем векторные представления (эмбединги), индексируя их в векторной базе данных FAISS, которая позволит нам эффективно искать сходство между векторами."
      ],
      "metadata": {
        "id": "GbvAwRYqsBLN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nH8jMZp_YCor"
      },
      "outputs": [],
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "# Обёртывание текста в объект Document\n",
        "documents = [Document(page_content=cleaned_text, metadata={\"source\": \"your_source\"})]\n",
        "\n",
        "# Разбиение\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=400,\n",
        "    chunk_overlap=50,\n",
        "    length_function=len\n",
        ")\n",
        "chunks = text_splitter.split_documents(documents)  # Список чанков\n",
        "\n",
        "print(f\"Текст разбит на {len(chunks)} чанков.\")\n",
        "print(\"Пример чанка:\")\n",
        "print(chunks[25].page_content[:1000] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Для создания из чанков индексируемых в FAISS эмбедингов мы будем использовать небольшую и эффективную модель семантического поиска **all-MiniLM-L6-v2**. Для последующего локального использования в API, мы сохраним базу данных локально"
      ],
      "metadata": {
        "id": "RhzSEQ5B2AH3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAhXYj7-arhH"
      },
      "outputs": [],
      "source": [
        "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name, model_kwargs={'device': device})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZ6QZJrza2m8"
      },
      "outputs": [],
      "source": [
        "# 4.3. Индексация в векторной базе данных (FAISS)\n",
        "vector_store = FAISS.from_documents(chunks, embeddings)\n",
        "\n",
        "# Сохраняем векторную базу\n",
        "vector_store_path = \"/tmp/faiss_index_KJB\"\n",
        "vector_store.save_local(vector_store_path)\n",
        "print(f\"Путь к хранилищу базы данных FAISS: {vector_store_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Настройка Большой Языковой Модели (LLM)**\n",
        "\n",
        "### Генерировать ответы мы будем с помощью сравнительно небольшой и при этом  мощной модели **gemma-7b-it**, работающей даже на CPU применяя 4-битную квантизацию, чтобы уместить модель в память GPU и ускорить её."
      ],
      "metadata": {
        "id": "CCOZKohkEmgM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAvup5OcbH3X"
      },
      "outputs": [],
      "source": [
        "# Конфигурация для 4-битной квантизации\n",
        "nf4_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Загрузка токенизатора и модели\n",
        "# llm_model_name = \"google/gemma-7b-it\"\n",
        "# llm_model_name = \"meta-llama/Llama-3-8B-Instruct\"\n",
        "llm_model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "# https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct\n",
        "print(f\"Загрузка токенизатора для {llm_model_name}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SuOpGr8qSM4I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Модель 'google/gemma-7b-it' является \"gated repo\" на Hugging Face. Для доступа к ней необходимо:\n",
        "1. Перейти на страницу модели на Hugging Face (https://huggingface.co/google/gemma-7b-it) и принять условия использования.\n",
        "2. Сгенерировать API токен в настройках Hugging Face (https://huggingface.co/settings/tokens).\n",
        "3. Добавить этот токен в секреты Colab с именем HF_TOKEN.\n",
        "4. Перезапустить сессию Colab (Runtime -> Restart session)."
      ],
      "metadata": {
        "id": "5K1795CJSAwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import torch\n",
        "# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "# from google.colab import drive\n",
        "\n",
        "# # --- 1. Монтирование Google Drive ---\n",
        "# # Это необходимо для доступа к вашим файлам на Google Drive\n",
        "# print(\"Монтирование Google Drive...\")\n",
        "# drive.mount('/content/drive')\n",
        "# print(\"Google Drive смонтирован.\")\n",
        "\n",
        "# # --- 2. Определение путей сохранения на Google Drive ---\n",
        "# # Выберите путь на вашем Google Drive, где будут храниться модели.\n",
        "# # Рекомендуется создать отдельную папку, например, 'colab_models'.\n",
        "# drive_base_path = '/content/drive/MyDrive/colab_models'\n",
        "\n",
        "# # Имя вашей LLM модели (например, \"google/gemma-7b-it\" или \"meta-llama/Llama-3-8B-Instruct\")\n",
        "# llm_model_name = \"google/gemma-7b-it\" # Укажите здесь вашу выбранную модель\n",
        "\n",
        "# # Создаем специфические пути для модели и токенизатора на Google Drive\n",
        "# model_save_path = os.path.join(drive_base_path, llm_model_name.replace(\"/\", \"_\") + \"_model\")\n",
        "# tokenizer_save_path = os.path.join(drive_base_path, llm_model_name.replace(\"/\", \"_\") + \"_tokenizer\")\n",
        "\n",
        "# # --- 3. Настройка квантизации (как у вас было) ---\n",
        "# nf4_config = BitsAndBytesConfig(\n",
        "#     load_in_4bit=True,\n",
        "#     bnb_4bit_quant_type=\"nf4\",\n",
        "#     bnb_4bit_use_double_quant=True,\n",
        "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
        "# )\n",
        "\n",
        "# # --- 4. Загрузка или скачивание модели и токенизатора ---\n",
        "# tokenizer = None\n",
        "# model_llm = None\n",
        "\n",
        "# # Проверяем, существует ли модель уже на Google Drive\n",
        "# if os.path.exists(model_save_path) and os.path.exists(tokenizer_save_path):\n",
        "#     print(f\"Модель и токенизатор '{llm_model_name}' найдены на Google Drive. Загрузка...\")\n",
        "#     try:\n",
        "#         tokenizer = AutoTokenizer.from_pretrained(tokenizer_save_path)\n",
        "#         model_llm = AutoModelForCausalLM.from_pretrained(\n",
        "#             model_save_path,\n",
        "#             quantization_config=nf4_config,\n",
        "#             torch_dtype=torch.bfloat16,\n",
        "#             device_map=\"auto\"\n",
        "#         )\n",
        "#         print(\"Модель и токенизатор успешно загружены с Google Drive.\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"Ошибка при загрузке с Google Drive: {e}. Попытка скачать из Hugging Face...\")\n",
        "#         # Если загрузка с Drive не удалась, удаляем неполные файлы и скачиваем заново\n",
        "#         if os.path.exists(model_save_path):\n",
        "#             os.rmdir(model_save_path) # Удалить, если была частичная загрузка\n",
        "#         if os.path.exists(tokenizer_save_path):\n",
        "#             os.rmdir(tokenizer_save_path) # Удалить, если была частичная загрузка\n",
        "\n",
        "#         tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
        "#         model_llm = AutoModelForCausalLM.from_pretrained(\n",
        "#             llm_model_name,\n",
        "#             quantization_config=nf4_config,\n",
        "#             torch_dtype=torch.bfloat16,\n",
        "#             device_map=\"auto\"\n",
        "#         )\n",
        "#         # После успешной загрузки из Hugging Face, сохраняем на Drive\n",
        "#         print(f\"Модель и токенизатор '{llm_model_name}' успешно скачаны. Сохранение на Google Drive...\")\n",
        "#         os.makedirs(model_save_path, exist_ok=True)\n",
        "#         os.makedirs(tokenizer_save_path, exist_ok=True)\n",
        "#         model_llm.save_pretrained(model_save_path)\n",
        "#         tokenizer.save_pretrained(tokenizer_save_path)\n",
        "#         print(\"Модель и токенизатор сохранены на Google Drive для будущего использования.\")\n",
        "# else:\n",
        "#     print(f\"Модель и токенизатор '{llm_model_name}' не найдены на Google Drive. Скачивание из Hugging Face...\")\n",
        "#     # Если на Drive нет, скачиваем из Hugging Face\n",
        "#     tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
        "#     model_llm = AutoModelForCausalLM.from_pretrained(\n",
        "#         llm_model_name,\n",
        "#         quantization_config=nf4_config,\n",
        "#         torch_dtype=torch.bfloat16,\n",
        "#         device_map=\"auto\"\n",
        "#     )\n",
        "#     # После успешной загрузки из Hugging Face, сохраняем на Drive\n",
        "#     print(f\"Модель и токенизатор '{llm_model_name}' успешно скачаны. Сохранение на Google Drive...\")\n",
        "#     os.makedirs(model_save_path, exist_ok=True)\n",
        "#     os.makedirs(tokenizer_save_path, exist_ok=True)\n",
        "#     model_llm.save_pretrained(model_save_path)\n",
        "#     tokenizer.save_pretrained(tokenizer_save_path)\n",
        "#     print(\"Модель и токенизатор сохранены на Google Drive для будущего использования.\")\n",
        "\n",
        "# # --- 5. Создание пайплайна (как у вас было) ---\n",
        "# text_generation_pipeline = pipeline(\n",
        "#     task=\"text-generation\",\n",
        "#     model=model_llm,\n",
        "#     tokenizer=tokenizer,\n",
        "#     max_new_tokens=256,\n",
        "#     do_sample=True,\n",
        "#     temperature=0.5, # Используйте свои последние настройки\n",
        "#     top_k=75,\n",
        "#     top_p=0.9,\n",
        "#     model_kwargs={\"quantization_config\": nf4_config, \"torch_dtype\": torch.bfloat16}\n",
        "# )\n",
        "\n",
        "# print(\"Настройка RAG-системы завершена.\")"
      ],
      "metadata": {
        "id": "3DOfH-A0SCS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Загружаем квантизировагнную модель LLM и создаём пайплайн HuggingFace для генерации текста ответа."
      ],
      "metadata": {
        "id": "Q42rJ3nhUrbq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYstlY4oQ1Pn"
      },
      "outputs": [],
      "source": [
        "# Загружаем модель с Hugging Face.\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "if hf_token:\n",
        "    os.environ[\"HF_TOKEN\"] = hf_token\n",
        "else:\n",
        "    print(\"Токен Hugging Face не найден\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
        "print(f\"Загрузка модели {llm_model_name} с 4-битной квантизацией...\")\n",
        "model_llm = AutoModelForCausalLM.from_pretrained(\n",
        "    llm_model_name,\n",
        "    quantization_config=nf4_config,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\") # Автозагрузка модели на доступное устройство (GPU/CPU)\n",
        "\n",
        "# Создание пайплайна\n",
        "text_generation_pipeline = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=llm_model_name,\n",
        "    tokenizer=tokenizer, # Remove tokenizer argument when passing model name\n",
        "    max_new_tokens=256, # Максимальное количество новых токенов в ответе\n",
        "    do_sample=True,\n",
        "   temperature=0.5, # Увеличено немного, для баланса между точностью и синтезом\n",
        "    top_k=75,        # Оставляем как было\n",
        "    top_p=0.9,       # Увеличено немного\n",
        "    # num_beams=5,\n",
        "    # repetition_penalty=1.1, # Для предотвращения повторений\n",
        "    model_kwargs={\"quantization_config\": nf4_config, \"torch_dtype\": torch.bfloat16} # передача конфигурации квантизации\n",
        ")\n",
        "print(\"HuggingFace Pipeline для генерации текста настроен.\")\n",
        "\n",
        "\n",
        "# Оборачиваем пайплайн в LangChain LLM\n",
        "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
        "print(\"LLM-пайплайн для LangChain настроен.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Конвейер RAG остоит из двух составляющих:\n",
        "\n",
        "### 1.  **Retrieval (Получение):** Извлечение на основе запроса пользователя релевантных текстовых фрагментов из базы знаний.\n",
        "### 2.  **Generation (Генерация):** Использование LLM для создания ответа, основываясь на извлеченном контексте.\n",
        "\n",
        "### Создаём промпт, инструктирующий LLM как ей необходимо отвечать на вопросы исходя из контекста, а также RAG-цепочку, связывающую векторный поиск с генерацией,  а также ретривер для извлечения релевантного контента из векторной базы данных (задаётся количество выдаваемых единиц контента).\n",
        "\n",
        "\n",
        "# Порядок операций в RAG-цепочке\n",
        "### 1. Получение вопроса\n",
        "### 2. Передаем вопроса ретриверу для извлечения контекста\n",
        "### 3. Формирование промпта с контекстом и вопросом\n",
        "### 4. Передача промпта в LLM для генерации ответа\n",
        "### 5. Парсинг выводимого LLM текста в строку"
      ],
      "metadata": {
        "id": "VsBZL1kDw_lT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8EK7xkGByNYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkJ9VirhTxFm"
      },
      "outputs": [],
      "source": [
        "# Промпт\n",
        "template = \"\"\"Используйте только следующий фрагмент контекста, чтобы ответить на вопрос пользователя.\n",
        "\n",
        "Инструкции:\n",
        "\n",
        "Извлеките все релевантные факты и информацию, относящиеся к вопросу.\n",
        "\n",
        "На основе этих фактов, сформулируйте краткий, точный и информативный ответ, что известно о той или иной личности, предмете, обстоятельствах, месте, действиях, о которых спрашивается.\n",
        "\n",
        "Если контекст не содержит достаточной информации для прямого ответа на вопрос, укажите это.\n",
        "\n",
        "Контекст:\n",
        "{context}\n",
        "\n",
        "Вопрос: {question}\n",
        "\n",
        "Ответ:\"\"\"\n",
        "\n",
        "RAG_PROMPT_TEMPLATE = PromptTemplate.from_template(template)\n",
        "\n",
        "# Создание RAG-цепочки (LangChain)\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Создаем ретривер из векторного хранилища\n",
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 7})\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "\n",
        "| RAG_PROMPT_TEMPLATE\n",
        "| llm\n",
        "| StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"Конвейер RAG успешно настроен.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Мы построили RAG систему. Самое время исппытать её и посмотреть что у нас получилось."
      ],
      "metadata": {
        "id": "_wyvs0S89uca"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYkrYiYn2HEi"
      },
      "outputs": [],
      "source": [
        "# --- 7. Тестирование RAG-системы ---\n",
        "\n",
        "print(\"\\n--- Тестирование RAG-системы ---\")\n",
        "\n",
        "# Вопрос 1: Вопрос, на который есть ответ в тексте\n",
        "question1 = \"Who was Uriah the Hittite?\"\n",
        "print(f\"Вопрос: {question1}\")\n",
        "response1 = rag_chain.invoke(question1)\n",
        "print(f\"Ответ: {response1}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Вопрос 2: Вопрос о персонаже\n",
        "question2 = \"Name of first son of Adam?\"\n",
        "print(f\"Вопрос: {question2}\")\n",
        "response2 = rag_chain.invoke(question2)\n",
        "print(f\"Ответ: {response2}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# # Вопрос 3: Вопрос, на который, возможно, нет прямого ответа в контексте\n",
        "question3 = \"Who is Moses?\"\n",
        "print(f\"Вопрос: {question3}\")\n",
        "response3 = rag_chain.invoke(question3)\n",
        "print(f\"Ответ: {response3}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# # Вопрос 4: Вопрос о деталях сюжета\n",
        "question4 = \"Who was the serpent?\"\n",
        "print(f\"Вопрос: {question4}\")\n",
        "response4 = rag_chain.invoke(question4)\n",
        "print(f\"Ответ: {response4}\")\n",
        "print(\"-\" * 50)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JXaZlwK2hM5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Пример 5: Вопрос о деталях сюжета\n",
        "question5 = \"Who was Nebuchadnezzar?\"\n",
        "print(f\"Вопрос: {question5}\")\n",
        "response5 = rag_chain.invoke(question5)\n",
        "print(f\"Ответ: {response5}\")\n",
        "print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwEgW4OFiUKQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Пример 5: Вопрос о деталях сюжета\n",
        "question5 = \"What was a name of Adam's wife\"\n",
        "print(f\"Вопрос: {question5}\")\n",
        "response5 = rag_chain.invoke(question5)\n",
        "print(f\"Ответ: {response5}\")\n",
        "print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вывод по тестированию RAG-системы:Система успешно извлекает релевантный контекст и генерирует ответы."
      ],
      "metadata": {
        "id": "G1J5DP71Ey7Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZU2IOHv9vZ4P"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}