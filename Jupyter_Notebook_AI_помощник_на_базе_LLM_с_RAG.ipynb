{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n"
      ],
      "metadata": {
        "id": "AQJOxhXffDRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Jupyter Notebook: AI-помощник на базе LLM с RAG для запросов к базе знаний**\n",
        "\n",
        "## **1. Введение**\n",
        "\n",
        "### Данный ноутбук посвящен разработке интеллектуального помощника, использующего большие языковые модели (LLM) и Retrieval-Augmented Generation (RAG) для ответов на вопросы на основе предоставленной базы знаний.\n",
        "\n",
        "### **Бизнес-проблема:** Компании часто имеют обширную внутреннюю документацию, к которой трудно получить быстрый и точный доступ. AI-помощник с RAG может предоставлять контекстно-зависимые ответы, улучшая эффективность и удовлетворенность.\n",
        "\n",
        "### **Цель проекта:** Построить RAG-систему, способную отвечать на вопросы на основе реального текстового корпуса, используя бесплатные ресурсы Google Colab."
      ],
      "metadata": {
        "id": "Crw6YSg7eqlQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOJp2rolzIQf"
      },
      "outputs": [],
      "source": [
        "# ## 2. установка и импорт необходимых библиотек, настройка среды\n",
        "# Здесь мы импортируем все библиотеки, которые потребуются для проекта.\n",
        "# Если ноутбук запускается в Google Colab, убедитесь, что используется среду с GPU (T4).\n",
        "\n",
        "!pip install transformers sentence-transformers langchain langchain-community faiss-cpu accelerate bitsandbytes -q\n",
        "!pip install pypdf -q # Для чтения PDF, если понадобится\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBo8sXg3Z8jj"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import os\n",
        "import requests # Для загрузки данных\n",
        "from tqdm.notebook import tqdm # Для прогресс-баров\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Для RAG\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.prompts import PromptTemplate\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "import torch\n",
        "from langchain_core.documents import Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6l2cM8TBXMVN"
      },
      "outputs": [],
      "source": [
        "# Проверяем доступность GPU\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU доступен: {torch.cuda.get_device_name(0)}\")\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    print(\"GPU не доступен, будет использоваться CPU.\")\n",
        "    device = \"cpu\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Для начала собираем данные для создаваемой нами базы знаний, которую мы сформируем из англоязычного текста Библии короля Якова (KJB), опубликованного на сайте проекта Project Gutenberg. Это большой корпус, состаящий из 39 книг, и он хорошо подходит в качестве коллекции документов различающихся стилем и манерой повествования. После этого производится очистка (предполагает удаление метаданных)\n",
        "\n",
        "### Источник данных: \"The King James Version of the Bible\" (Библия короля Якова)\n",
        "### URL: https://www.gutenberg.org/files/10/10-0.txt"
      ],
      "metadata": {
        "id": "D6EQ5pMThF5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Загрузка текста книги из Project Gutenberg...\")\n",
        "book_url = \"https://www.gutenberg.org/files/10/10-0.txt\"\n",
        "response = requests.get(book_url)\n",
        "response.raise_for_status() # Проверка, успешности запроса\n",
        "\n",
        "raw_text = response.text\n",
        "print(f\"Текст успешно загружен. Размер текста: {len(raw_text)} символов.\")\n",
        "\n",
        "# Очистка текста: удаляем метаданные Project Gutenberg в начале и конце\n",
        "start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK 10 ***\"\n",
        "end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK 10 ***\"\n",
        "\n",
        "start_index = raw_text.find(start_marker)\n",
        "end_index = raw_text.find(end_marker)\n",
        "\n",
        "if start_index!= -1 and end_index!= -1:\n",
        "    cleaned_text = raw_text[start_index + len(start_marker):end_index].strip()\n",
        "    print(f\"Текст очищен от метаданных. Размер очищенного текста: {len(cleaned_text)} символов.\")\n",
        "else:\n",
        "    cleaned_text = raw_text.strip()\n",
        "    print(\"Метаданные не найдены, используется весь текст.\")\n",
        "\n",
        "# Сохраняем очищенный текст в файл для дальнейшей обработки\n",
        "with open(\"KJB.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(cleaned_text)\n",
        "print(\"Очищенный текст сохранен в 'KJB.txt'.\")"
      ],
      "metadata": {
        "id": "HyQ-_ZFMhB9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Предварительная обработка текста и генерация эмбеддингов\n",
        "\n",
        "## На этом этапе мы подготовим текстовые данные для RAG-системы:\n",
        "## - **Разбиение на чанки (Chunking):**\n",
        "## - **Генерация эмбеддингов:**\n",
        "## - **Индексация эмбеддингов в векторной базе данных FAISS**"
      ],
      "metadata": {
        "id": "GbvAwRYqsBLN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nH8jMZp_YCor"
      },
      "outputs": [],
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "# Обёртывание текста в объект Document\n",
        "documents = [Document(page_content=cleaned_text, metadata={\"source\": \"your_source\"})]\n",
        "\n",
        "# Разбиение\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=100,\n",
        "    length_function=len\n",
        ")\n",
        "chunks = text_splitter.split_documents(documents)  # Список чанков\n",
        "\n",
        "print(f\"Текст разбит на {len(chunks)} чанков.\")\n",
        "print(\"Пример чанка:\")\n",
        "print(chunks[25].page_content[:1000] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Создаём из чанков эмбединги с использованием небольшой и эффективной модели семантического поиска **all-MiniLM-L6-v2** и индексируем в базе данных FAISS, которая позволяет нам эффективно искать сходство между векторами."
      ],
      "metadata": {
        "id": "RhzSEQ5B2AH3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAhXYj7-arhH"
      },
      "outputs": [],
      "source": [
        "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name, model_kwargs={'device': device})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZ6QZJrza2m8"
      },
      "outputs": [],
      "source": [
        "# 4.3. Индексация в векторной базе данных (FAISS)\n",
        "vector_store = FAISS.from_documents(chunks, embeddings)\n",
        "\n",
        "# Сохраняем векторную базу данных локально (для использования в API)\n",
        "vector_store_path = \"/tmp/faiss_index_KJB\"\n",
        "vector_store.save_local(vector_store_path)\n",
        "print(f\"Путь до векторной базы данных FAISS: {vector_store_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Настройка Большой Языковой Модели (LLM)\n",
        "\n",
        "### Генерировать ответы мы будем с помощью небольшой и мощной модели **gemma-2b-it**, работающей даже на CPU применяя 4-битную квантизацию, чтобы уместить модель в память GPU и ускорить её."
      ],
      "metadata": {
        "id": "CCOZKohkEmgM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAvup5OcbH3X"
      },
      "outputs": [],
      "source": [
        "# Конфигурация для 4-битной квантизации\n",
        "nf4_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Загрузка токенизатора и модели\n",
        "llm_model_name = \"google/gemma-2b-it\"\n",
        "print(f\"Загрузка токенизатора для {llm_model_name}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SuOpGr8qSM4I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Модель 'google/gemma-2b-it' является \"gated repo\" на Hugging Face. Для доступа к ней необходимо:\n",
        "1. Перейти на страницу модели на Hugging Face (https://huggingface.co/google/gemma-2b-it) и принять условия использования.\n",
        "2. Сгенерировать API токен в настройках Hugging Face (https://huggingface.co/settings/tokens).\n",
        "3. Добавить этот токен в секреты Colab с именем HF_TOKEN.\n",
        "4. Перезапустить сессию Colab (Runtime -> Restart session)."
      ],
      "metadata": {
        "id": "5K1795CJSAwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Загрузка токена из секретов Colab и установка его как переменной окружения для аутентификации при загрузке gated моделей с Hugging Face.\n",
        "# from google.colab import userdata\n",
        "# import os\n",
        "\n",
        "# hf_token = userdata.get('HF_TOKEN')\n",
        "# if hf_token:\n",
        "#     os.environ[\"HF_TOKEN\"] = hf_token\n",
        "#     print(\"Hugging Face token loaded from Colab secrets.\")\n",
        "#     if \"HF_TOKEN\" in os.environ:\n",
        "#         print(\"HF_TOKEN environment variable is set.\")\n",
        "# else:\n",
        "#     print(\"Hugging Face token not found in Colab secrets. Please add it as HF_TOKEN.\")\n",
        "#     print(\"HF_TOKEN environment variable is NOT set.\")\n",
        "\n",
        "\n",
        "# # tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
        "# # print(f\"Загрузка модели {llm_model_name} с 4-битной квантизацией...\")\n",
        "# # model_llm = AutoModelForCausalLM.from_pretrained(\n",
        "# #     llm_model_name,\n",
        "# #     quantization_config=nf4_config,\n",
        "# #     torch_dtype=torch.bfloat16,\n",
        "# #     device_map=\"auto\" # Автоматически распределяет модель по доступным устройствам (GPU/CPU)\n",
        "# # )\n",
        "# # print(\"Модель LLM загружена.\")\n",
        "\n",
        "# # Создание HuggingFace Pipeline для генерации текста\n",
        "# # Pass the model name string to the pipeline\n",
        "# text_generation_pipeline = pipeline(\n",
        "#     task=\"text-generation\", # Explicitly specify the task\n",
        "#     model=llm_model_name, # Pass the model name string\n",
        "#     # tokenizer=tokenizer, # Remove tokenizer argument when passing model name\n",
        "#     max_new_tokens=256, # Максимальное количество новых токенов в ответе\n",
        "#     do_sample=True,\n",
        "#     temperature=0.7,\n",
        "#     top_k=50,\n",
        "#     top_p=0.95,\n",
        "#     repetition_penalty=1.1, # Для предотвращения повторений\n",
        "#     model_kwargs={\"quantization_config\": nf4_config, \"torch_dtype\": torch.bfloat16} # Pass quantization config via model_kwargs\n",
        "# )\n",
        "# print(\"HuggingFace Pipeline для генерации текста настроен.\")\n",
        "\n",
        "\n",
        "# # Оборачиваем пайплайн в LangChain LLM\n",
        "# llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
        "# print(\"LLM-пайплайн для LangChain настроен.\")"
      ],
      "metadata": {
        "id": "3DOfH-A0SCS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Загружаем квантизировагнную модель LLM на доступное устройство, далее создаём пайплайн HuggingFace для генерации текста ответа."
      ],
      "metadata": {
        "id": "Q42rJ3nhUrbq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7cMsR0vVsGu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYstlY4oQ1Pn"
      },
      "outputs": [],
      "source": [
        "# Загружаем модель с Hugging Face.\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "if hf_token:\n",
        "    os.environ[\"HF_TOKEN\"] = hf_token\n",
        "else:\n",
        "    print(\"Токен Hugging Face не найден\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
        "print(f\"Загрузка модели {llm_model_name} с 4-битной квантизацией...\")\n",
        "model_llm = AutoModelForCausalLM.from_pretrained(\n",
        "    llm_model_name,\n",
        "    quantization_config=nf4_config,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\") # Автозагрузка модели на доступное устройство (GPU/CPU)\n",
        "\n",
        "# Создание пайплайна\n",
        "text_generation_pipeline = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=llm_model_name,\n",
        "    # tokenizer=tokenizer, # Remove tokenizer argument when passing model name\n",
        "    max_new_tokens=64, # Максимальное количество новых токенов в ответе\n",
        "    do_sample=True,\n",
        "    temperature=0.3,\n",
        "    top_k=75,\n",
        "    top_p=0.97,\n",
        "    repetition_penalty=1.1, # Для предотвращения повторений\n",
        "    model_kwargs={\"quantization_config\": nf4_config, \"torch_dtype\": torch.bfloat16} # передача конфигурации квантизации\n",
        ")\n",
        "print(\"HuggingFace Pipeline для генерации текста настроен.\")\n",
        "\n",
        "\n",
        "# Оборачиваем пайплайн в LangChain LLM\n",
        "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
        "print(\"LLM-пайплайн для LangChain настроен.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Конвейер RAG остоит из двух составляющих:\n",
        "\n",
        "### 1.  **Retrieval (Получение):** Извлечение на основе запроса пользователя релевантных текстовых фрагментов из базы знаний.\n",
        "### 2.  **Generation (Генерация):** Использование LLM для создания ответа, основываясь на извлеченном контексте.\n",
        "\n",
        "### Создаём промпт, инструктирующий LLM как ей необходимо отвечать на вопросы исходя из контекста, а также RAG-цепочку, связывающую векторный поиск с генерацией,  а также ретривер для извлечения релевантного контента из векторной базы данных (задаётся количество выдаваемых единиц контента).\n",
        "\n",
        "\n",
        "# Порядок операций в RAG-цепочке\n",
        "### 1. Получение вопроса\n",
        "### 2. Передаем вопроса ретриверу для извлечения контекста\n",
        "### 3. Формирование промпта с контекстом и вопросом\n",
        "### 4. Передача промпта в LLM для генерации ответа\n",
        "### 5. Парсинг выводимого LLM текста в строку"
      ],
      "metadata": {
        "id": "VsBZL1kDw_lT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8EK7xkGByNYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkJ9VirhTxFm"
      },
      "outputs": [],
      "source": [
        "# Промпт\n",
        "template = \"\"\"Используй следующий контекст, чтобы ответить на вопрос пользователя.\n",
        "Если ты не знаешь ответа, просто скажи, что не можешь найти информацию в предоставленном контексте.\n",
        "Не придумывай ответ если не знаешь.\n",
        "\n",
        "Контекст:\n",
        "{context}\n",
        "\n",
        "Вопрос: {question}\n",
        "\n",
        "Ответ:\"\"\"\n",
        "\n",
        "RAG_PROMPT_TEMPLATE = PromptTemplate.from_template(template)\n",
        "\n",
        "# Создание RAG-цепочки (LangChain)\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Создаем ретривер из векторного хранилища\n",
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 23})\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "\n",
        "| RAG_PROMPT_TEMPLATE\n",
        "| llm\n",
        "| StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"Конвейер RAG успешно настроен.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Мы построили RAG систему. Самое время исппытать её и посмотреть что у нас получилось."
      ],
      "metadata": {
        "id": "_wyvs0S89uca"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYkrYiYn2HEi"
      },
      "outputs": [],
      "source": [
        "# --- 7. Тестирование RAG-системы ---\n",
        "\n",
        "print(\"\\n--- Тестирование RAG-системы ---\")\n",
        "\n",
        "# Вопрос 1: Вопрос, на который есть ответ в тексте\n",
        "question1 = \"Why James decided to runaway?\"\n",
        "print(f\"Вопрос: {question1}\")\n",
        "response1 = rag_chain.invoke(question1)\n",
        "print(f\"Ответ: {response1}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Вопрос 2: Вопрос о персонаже\n",
        "question2 = \"What were the names of Adam's' sons?\"\n",
        "print(f\"Вопрос: {question2}\")\n",
        "response2 = rag_chain.invoke(question2)\n",
        "print(f\"Ответ: {response2}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# # Вопрос 3: Вопрос, на который, возможно, нет прямого ответа в контексте\n",
        "question3 = \"Who is Moses?\"\n",
        "print(f\"Вопрос: {question3}\")\n",
        "response3 = rag_chain.invoke(question3)\n",
        "print(f\"Ответ: {response3}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# # Вопрос 4: Вопрос о деталях сюжета\n",
        "question4 = \"Who was the serpent?\"\n",
        "print(f\"Вопрос: {question4}\")\n",
        "response4 = rag_chain.invoke(question4)\n",
        "print(f\"Ответ: {response4}\")\n",
        "print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JXaZlwK2hM5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Пример 5: Вопрос о деталях сюжета\n",
        "question5 = \"What was Nebuchadnezzar?\"\n",
        "print(f\"Вопрос: {question5}\")\n",
        "response5 = rag_chain.invoke(question5)\n",
        "print(f\"Ответ: {response5}\")\n",
        "print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwEgW4OFiUKQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Пример 5: Вопрос о деталях сюжета\n",
        "question5 = \"What was a name of Adam's wife\"\n",
        "print(f\"Вопрос: {question5}\")\n",
        "response5 = rag_chain.invoke(question5)\n",
        "print(f\"Ответ: {response5}\")\n",
        "print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7f_mHmL2i-a"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"\\nВывод по тестированию RAG-системы:\")\n",
        "print(\" - Система успешно извлекает релевантный контекст и генерирует ответы.\")\n",
        "print(\" - Для вопросов вне контекста базы знаний, LLM должна отвечать, что не может найти информацию.\")\n",
        "\n",
        "#\n",
        "# ## 8. Оценка и оптимизация RAG-системы (концептуально)\n",
        "#\n",
        "# Оценка RAG-систем — это сложная задача, требующая как традиционных метрик NLP,\n",
        "# так и метрик, специфичных для качества генерации и получения информации.\n",
        "#\n",
        "# **Метрики оценки (концептуально):**\n",
        "# -   **Relevance (Релевантность):** Насколько ответ релевантен запросу.\n",
        "# -   **Faithfulness (Достоверность):** Насколько ответ основан на предоставленном контексте.\n",
        "# -   **Context Precision/Recall (Точность/Полнота контекста):** Насколько извлеченный контекст\n",
        "#     был релевантен и полон.\n",
        "# -   **Answer Correctness (Правильность ответа):** Насколько ответ верен фактически.\n",
        "# -   **Fluency (Беглость), Coherence (Связность).**\n",
        "#\n",
        "# **Инструменты:** RAGAS (Rag-as-a-Service), TruLens, LangChain Evaluators (требуют размеченных данных).\n",
        "#\n",
        "# **Оптимизация:** Экспериментирование с:\n",
        "# -   Различными стратегиями разбиения на чанки (размер, перекрытие).\n",
        "# -   Моделями эмбеддингов.\n",
        "# -   Порогами поиска в векторной базе данных (параметр `k` в ретривере).\n",
        "# -   Шаблонами промптов для LLM.\n",
        "# -   Моделями LLM (использование более крупных или специализированных моделей).\n",
        "#\n",
        "# Для полноценной оценки требуются размеченные пары \"вопрос-ответ\" с указанием релевантных фрагментов текста.\n",
        "# В рамках этого ноутбука мы проводим только качественную оценку на примерах.\n",
        "\n",
        "print(\"\\n--- Концепция оценки и оптимизации конвейера RAG ---\")\n",
        "print(\"Для полноценной оценки RAG-системы требуются размеченные данные (вопросы, правильные ответы, релевантные чанки).\")\n",
        "print(\"Оптимизация включает эксперименты с параметрами разбиения на чанки, моделями эмбеддингов, LLM и промптами.\")\n",
        "\n",
        "#\n",
        "# ## 9. Сохранение артефактов для развертывания\n",
        "#\n",
        "# Для развертывания RAG-системы в виде сервиса нам нужно сохранить обученные компоненты:\n",
        "# -   Векторную базу данных FAISS.\n",
        "# -   Название модели эмбеддингов (для загрузки в сервисе).\n",
        "# -   Название LLM (для загрузки в сервисе).\n",
        "# -   Шаблон промпта.\n",
        "\n",
        "# Векторная база данных уже сохранена в разделе 4: /tmp/faiss_index_alice\n",
        "# model_llm и tokenizer будут загружены по имени в сервисе.\n",
        "# embeddings также будут загружены по имени.\n",
        "\n",
        "# Сохраняем информацию о моделях для сервиса\n",
        "rag_config = {\n",
        "    \"embedding_model_name\": embedding_model_name,\n",
        "    \"llm_model_name\": llm_model_name,\n",
        "    \"vector_store_path\": vector_store_path,\n",
        "    \"prompt_template\": template\n",
        "}\n",
        "\n",
        "import json\n",
        "with open(\"/tmp/rag_config.json\", \"w\") as f:\n",
        "    json.dump(rag_config, f, indent=4)\n",
        "print(\"Конфигурация RAG-системы сохранена в '/tmp/rag_config.json'.\")\n",
        "\n",
        "print(\"\\nВывод по сохранению артефактов:\")\n",
        "print(\" - Векторная база данных FAISS и конфигурация RAG-системы сохранены.\")\n",
        "print(\" - Эти файлы будут использоваться для инициализации RAG-системы в FastAPI, Gradio и Streamlit сервисах.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZU2IOHv9vZ4P"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}